{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Implementing Inter-Annotator Agreement\n",
    "- #### Export the annotations in either JSON or CSV files.\n",
    "> We have exported the annotations from label-studio as csv, and then __processed__ it to match the schema given as example [here](https://docs.google.com/spreadsheets/d/1yOrKVgBYuam2MqqKPXY2wMBLTlx_VtlXpUQdXXW18LM/edit?gid=1503756051#gid=1503756051) by sir.\n",
    "\n",
    "_Example_:   \n",
    "__Sentence__: इन दोनों Freight Corridor के इर्द गिर्द दिल्ली-मुंबई Industrial Corridor और अमृतसर-कोलकाता Industrial Corridor भी विकसित किए जा रहे हैं।   \n",
    "__POS Tags__: [{'word': 'इन', 'entity': 'DET'}, {'word': 'दोनों', 'entity': 'DET'}, {'word': 'Freight Corridor', 'entity': 'PROPN'}, {'word': 'के', 'entity': 'ADP'}, {'word': 'इर्द गिर्द', 'entity': 'ADP'}, {'word': 'और', 'entity': 'CONJ'}, {'word': 'भी', 'entity': 'PART'}, {'word': 'विकसित', 'entity': 'VERB'}, {'word': 'किए', 'entity': 'VERB'}, {'word': 'दिल्ली-मुंबई Industrial Corridor', 'entity': 'PROPN'}, {'word': 'अमृतसर-कोलकाता Industrial Corridor', 'entity': 'PROPN'}, {'word': 'हैं', 'entity': 'NOUN'}, {'word': 'जा रहे', 'entity': 'VERB'}, {'word': '।', 'entity': 'X'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv(\"NLP_POS_480_499_Romit.csv\")\n",
    "\n",
    "# Initialize a list to store the formatted output\n",
    "formatted_data = []\n",
    "\n",
    "# Loop through each row in the dataframe\n",
    "for _, row in df.iterrows():\n",
    "    # Extract the sentence (from \"label\" column)\n",
    "    labels = json.loads(row['label'])\n",
    "    \n",
    "    # Create the POS tag list for the sentence\n",
    "    pos_tags = [{'word': item['text'], 'entity': item['labels'][0]} for item in labels]\n",
    "    \n",
    "    # Extract sentence text\n",
    "    sentence = row['text']\n",
    "    \n",
    "    # Append the result to the list\n",
    "    formatted_data.append([sentence, pos_tags])\n",
    "\n",
    "# Convert the list into a new DataFrame\n",
    "output_df = pd.DataFrame(formatted_data, columns=[\"Sentences\", \"POS Tags\"])\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "output_df.to_csv(\"NLP_POS_480_499_Romit_processed.csv\", index=False)\n",
    "\n",
    "print(\"Output saved to output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Using Python code, calculate Cohen’s Kappa & Fleiss Kappa.\n",
    "    - Use Cohen’s Kappa for the NLP Dataset Task.  \n",
    "    - Use Fleiss Kappa for the CV Dataset Task. \n",
    "    - Get the third annotation from any other teams and then calculate the Fleiss Kappa.\n",
    "\n",
    "> We are using sklearn's `cohen_kappa_score` function to calculate the cohen kappa between 2 arrays, which contain the tags given to the words respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [{'word': 'इन', 'entity': 'DET'}, {'word': 'दो...\n",
       "1     [{'word': 'उत्तर प्रदेश', 'entity': 'PROPN'}, ...\n",
       "2     [{'word': '।', 'entity': 'X'}, {'word': 'इसके'...\n",
       "3     [{'word': 'अर्णव', 'entity': 'PROPN'}, {'word'...\n",
       "4     [{'word': 'देखा', 'entity': 'VERB'}, {'word': ...\n",
       "5     [{'word': 'त्रिपुरा', 'entity': 'PROPN'}, {'wo...\n",
       "6     [{'word': 'जम्मू-कश्मीर', 'entity': 'PROPN'}, ...\n",
       "7     [{'word': 'श्रीमती विजया लक्ष्मी पंडित', 'enti...\n",
       "8     [{'word': 'इसके', 'entity': 'DET'}, {'word': '...\n",
       "9     [{'word': 'यहां', 'entity': 'ADV'}, {'word': '...\n",
       "10    [{'word': 'Army', 'entity': 'PROPN'}, {'word':...\n",
       "11    [{'word': 'साथियो', 'entity': 'NOUN'}, {'word'...\n",
       "12    [{'word': 'अमेरिका', 'entity': 'PROPN'}, {'wor...\n",
       "13    [{'word': 'अब', 'entity': 'ADV'}, {'word': 'II...\n",
       "14    [{'word': 'इसे', 'entity': 'PRON'}, {'word': '...\n",
       "15    [{'word': 'अफगानिस्तान', 'entity': 'PROPN'}, {...\n",
       "16    [{'word': 'हमारे', 'entity': 'PRON'}, {'word':...\n",
       "17    [{'word': '“', 'entity': 'X'}, {'word': ',', '...\n",
       "18    [{'word': 'कुछ', 'entity': 'DET'}, {'word': 'द...\n",
       "19    [{'word': 'विदेश', 'entity': 'NOUN'}, {'word':...\n",
       "Name: POS Tags, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 1 column 3 (char 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m annotations2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNLP_POS_480_499_Rudra.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOS Tags\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# annotator1_words, annotator1_tags = [(item['word'], item['entity']) for item in annotations1]\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m annotations1])\n\u001b[0;32m      7\u001b[0m annotator1_tags\n",
      "File \u001b[1;32mc:\\Users\\romit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\romit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\romit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)"
     ]
    }
   ],
   "source": [
    "# Load the annotations from CSV files for annotator 1 (Romit) and annotator 2 (Rudra)\n",
    "annotations1 = pd.read_csv('NLP_POS_480_499_Romit.csv')['POS Tags']\n",
    "annotations2 = pd.read_csv('NLP_POS_480_499_Rudra.csv')['POS Tags']\n",
    "\n",
    "# annotator1_words, annotator1_tags = [(item['word'], item['entity']) for item in annotations1]\n",
    "print([json.loads(str(item)) for item in annotations1])\n",
    "annotator1_tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
